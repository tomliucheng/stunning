#!/usr/bin/python

#_*_ conding:utf-8 _*_

import urllib2 request
import cookielib

url = "http://www.baidu.com"

print("one:")
response1 = request.urlopen(url)
print(response1.getcode()) #打印打开网页的返回状态，2000 表示ok
print(len(response1.read()))


print("two:")
request1 = request.Request(url)
request1.add_header('user-agent','Mozilla/5.0')
response2 = request.urlopen(url)
print(response2.getcode()) #打印打开网页的返回状态，2000 表示ok
print(len(response2.read()))


print("three:")
cj = http.cookiejar.CookieJar()
opener = request.build_opener(request.HTTPCookieProcessor(cj))
request.install_opener(opener)
response3 = request.urlopen(url)
print(response3.getcode()) #打印打开网页的返回状态，2000 表示ok
print(len(response3.read()))
print(cj)


#beautifulsoup4 to parses html DOM:document object module
from bs4 import BeautifulSoup
import re

html_doc = "" #引用段落自http://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree，其中的短文，因笔记字数要求有限，无法直接复制过来

soup = BeautifulSoup(html_doc, 'html.parser', from_encoding="utf-8")
print u"获取所有的链接"
links = soup.find_all('a')
for link in links:
print link.name, link['href'], link.get_text()

print u"获取lacie的链接"
link_node = soup.find('a', href="http://example.com/lacie")
print link_node.name, link_node['href'], link_node.get_text()

print u"正则匹配"
link_node = soup.find('a', href=re.compile(r"ill"))
print link_node.name, link_node['href'], link_node.get_text()

print u"获取p段落文字"
p_node = soup.find('p', class_="title")
print p_node.name, p_node.get_text()
